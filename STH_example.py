#!/usr/bin/env python3
"""
STHæ¨¡å‹ä½¿ç”¨ç¤ºä¾‹
å±•ç¤ºå¦‚ä½•ä½¿ç”¨æ—¶ç©ºæ··åˆæ³¨æ„åŠ›ç½‘ç»œ+åˆ†å±‚PPOè¿›è¡Œäº¤é€šä¿¡å·æ§åˆ¶
"""

import numpy as np
import torch
import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from STH_model import STHANModel
from STH_ppo import HierarchicalPPO
from STH_agent import STHAgent
from STH_config import get_config

class MockTrafficEnvironment:
    """
    æ¨¡æ‹Ÿäº¤é€šç¯å¢ƒï¼Œç”¨äºæ¼”ç¤ºSTHæ¨¡å‹çš„ä½¿ç”¨
    åœ¨å®é™…åº”ç”¨ä¸­ï¼Œè¿™é‡Œåº”è¯¥æ›¿æ¢ä¸ºçœŸå®çš„äº¤é€šä»¿çœŸç¯å¢ƒï¼ˆå¦‚SUMOï¼‰
    """
    def __init__(self, num_agents=4, obs_dim=16, action_dim=4):
        self.num_agents = num_agents
        self.obs_dim = obs_dim
        self.action_dim = action_dim
        self.step_count = 0
        self.max_steps = 100
        
    def reset(self):
        """é‡ç½®ç¯å¢ƒ"""
        self.step_count = 0
        # è¿”å›éšæœºåˆå§‹çŠ¶æ€
        return np.random.randn(self.num_agents, self.obs_dim)
    
    def step(self, actions):
        """
        æ‰§è¡ŒåŠ¨ä½œ
        
        Args:
            actions: åŠ¨ä½œæ•°ç»„ [num_agents]
            
        Returns:
            obs: æ–°çš„è§‚æµ‹
            reward: å¥–åŠ±
            done: æ˜¯å¦ç»“æŸ
            info: é¢å¤–ä¿¡æ¯
        """
        self.step_count += 1
        
        # æ¨¡æ‹ŸçŠ¶æ€è½¬ç§»
        obs = np.random.randn(self.num_agents, self.obs_dim)
        
        # æ¨¡æ‹Ÿå¥–åŠ±ï¼ˆåŸºäºåŠ¨ä½œçš„ç®€å•å¥–åŠ±ï¼‰
        reward = -np.abs(actions - 2)  # åå¥½åŠ¨ä½œ2
        
        # æ¨¡æ‹Ÿäº¤é€šæŒ‡æ ‡
        info = {
            'queue_length': np.random.randn(self.num_agents),
            'waiting_time': np.random.randn(self.num_agents),
            'throughput': np.random.randn(self.num_agents),
            'pressure': np.random.randn(self.num_agents),
            'avg_speed': np.random.randn(self.num_agents),
            'phase_change': np.random.randint(0, 2, self.num_agents),
            'neighbor_coordination': np.random.randn()
        }
        
        # æ£€æŸ¥æ˜¯å¦ç»“æŸ
        done = np.array([self.step_count >= self.max_steps] * self.num_agents)
        
        return obs, reward, done, info

def example_basic_usage():
    """åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹"""
    print("=" * 60)
    print("STH Model Basic Usage Example")
    print("=" * 60)
    
    # 1. è·å–é…ç½®ï¼ˆä½¿ç”¨æµå—3x4æ•°æ®é›†ï¼‰
    config = get_config('small', dataset_name='jinan_3_4')
    print(f"Using config: small network with Jinan 3x4 dataset ({config['num_agents']} agents)")
    
    # 2. åˆ›å»ºæ¨¡æ‹Ÿç¯å¢ƒ
    env = MockTrafficEnvironment(
        num_agents=config['num_agents'],
        obs_dim=config['obs_dim'],
        action_dim=config['action_dim']
    )
    
    # 3. åˆ›å»ºSTHæ™ºèƒ½ä½“ï¼ˆä½¿ç”¨æµå—3x4æ•°æ®é›†ï¼‰
    agent = STHAgent(env, config_name='default', dataset_name='jinan_3_4')
    
    # 4. è¿è¡Œå‡ ä¸ªå›åˆè¿›è¡Œæ¼”ç¤º
    print("\nRunning demonstration episodes...")
    for episode in range(3):
        print(f"\nEpisode {episode + 1}:")
        
        # è¿è¡Œä¸€ä¸ªå›åˆ
        episode_info = agent.run_episode(training=True)
        
        print(f"  Total Reward: {episode_info['total_reward']:.2f}")
        print(f"  Steps: {episode_info['step_count']}")
        print(f"  Avg Reward: {episode_info['avg_reward']:.2f}")
        
        # æ˜¾ç¤ºäº¤é€šæŒ‡æ ‡
        if episode_info['traffic_metrics']:
            print("  Traffic Metrics:")
            for key, value in episode_info['traffic_metrics'].items():
                print(f"    {key}: {value:.2f}")
    
    # 5. è·å–è®­ç»ƒç»Ÿè®¡
    stats = agent.get_training_stats()
    print(f"\nTraining Statistics:")
    for key, value in stats.items():
        print(f"  {key}: {value:.2f}")
    
    print("\nâœ“ Basic usage example completed!")

def example_model_inference():
    """æ¨¡å‹æ¨ç†ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("STH Model Inference Example")
    print("=" * 60)
    
    config = get_config('small', dataset_name='jinan_3_4')
    
    # åˆ›å»ºæ¨¡å‹
    model = STHANModel(
        obs_dim=config['obs_dim'],
        action_dim=config['action_dim'],
        num_agents=config['num_agents'],
        history_len=config['history_len'],
        embed_dim=config['embed_dim'],
        n_heads=config['n_heads']
    )
    
    # åˆ›å»ºPPO
    ppo = HierarchicalPPO(model, config)
    
    # å‡†å¤‡è¾“å…¥æ•°æ®
    batch_size = 1
    obs = np.random.randn(batch_size, config['num_agents'], config['obs_dim'])
    neighbor_obs = np.random.randn(batch_size, config['num_agents'], config['num_neighbors'], config['obs_dim'])
    history_obs = np.random.randn(batch_size, config['num_agents'], config['history_len'], config['obs_dim'])
    
    print(f"Input shapes:")
    print(f"  obs: {obs.shape}")
    print(f"  neighbor_obs: {neighbor_obs.shape}")
    print(f"  history_obs: {history_obs.shape}")
    
    # è¿›è¡Œæ¨ç†
    with torch.no_grad():
        high_level_actions, low_level_actions, high_level_log_probs, low_level_log_probs, value = \
            ppo.select_action(obs, neighbor_obs, history_obs, training=False)
    
    print(f"\nOutput shapes:")
    print(f"  high_level_actions: {high_level_actions.shape}")
    print(f"  low_level_actions: {low_level_actions.shape}")
    print(f"  value: {value.shape}")
    
    print(f"\nSelected actions:")
    print(f"  High-level: {high_level_actions[0]}")
    print(f"  Low-level: {low_level_actions[0]}")
    print(f"  Value: {value[0, :, 0]}")
    
    print("\nâœ“ Model inference example completed!")

def example_custom_config():
    """è‡ªå®šä¹‰é…ç½®ç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("STH Model Custom Configuration Example")
    print("=" * 60)
    
    # è·å–é»˜è®¤é…ç½®ï¼ˆä½¿ç”¨æµå—3x4æ•°æ®é›†ï¼‰
    config = get_config('default', dataset_name='jinan_3_4')
    
    # è‡ªå®šä¹‰é…ç½®
    custom_config = config.copy()
    custom_config.update({
        'num_agents': 6,
        'obs_dim': 20,
        'action_dim': 6,
        'embed_dim': 256,
        'n_heads': 8,
        'num_layers': 3,
        'history_len': 8,
        'ppo_epochs': 15,
        'batch_size': 128,
        'lr': 5e-4,
        # è‡ªå®šä¹‰å¥–åŠ±å¡‘å½¢å‚æ•°
        'queue_length_weight': -0.15,
        'waiting_time_weight': -0.08,
        'throughput_weight': 0.25,
        'pressure_weight': -0.2,
        'coordination_weight': 0.15,
    })
    
    print("Custom configuration:")
    for key, value in custom_config.items():
        if key in ['queue_length_weight', 'waiting_time_weight', 'throughput_weight', 
                  'pressure_weight', 'coordination_weight']:
            print(f"  {key}: {value}")
    
    # åˆ›å»ºæ¨¡å‹
    model = STHANModel(
        obs_dim=custom_config['obs_dim'],
        action_dim=custom_config['action_dim'],
        num_agents=custom_config['num_agents'],
        history_len=custom_config['history_len'],
        embed_dim=custom_config['embed_dim'],
        n_heads=custom_config['n_heads'],
        num_layers=custom_config['num_layers']
    )
    
    print(f"\nModel created with {sum(p.numel() for p in model.parameters()):,} parameters")
    
    print("\nâœ“ Custom configuration example completed!")

def example_training_workflow():
    """è®­ç»ƒå·¥ä½œæµç¤ºä¾‹"""
    print("\n" + "=" * 60)
    print("STH Model Training Workflow Example")
    print("=" * 60)
    
    # åˆ›å»ºç¯å¢ƒï¼ˆä½¿ç”¨æµå—3x4æ•°æ®é›†é…ç½®ï¼‰
    config = get_config('fast', dataset_name='jinan_3_4')
    env = MockTrafficEnvironment(
        num_agents=config['num_agents'],
        obs_dim=config['obs_dim'], 
        action_dim=config['action_dim']
    )
    
    # åˆ›å»ºæ™ºèƒ½ä½“ï¼ˆä½¿ç”¨æµå—3x4æ•°æ®é›†ï¼‰
    agent = STHAgent(env, config_name='fast', dataset_name='jinan_3_4')
    
    print("Training workflow:")
    print("1. Initialize agent âœ“")
    print("2. Run training episodes...")
    
    # è¿è¡Œå‡ ä¸ªè®­ç»ƒå›åˆ
    for episode in range(5):
        episode_info = agent.run_episode(training=True)
        
        if episode % 2 == 0:
            print(f"   Episode {episode + 1}: Reward = {episode_info['total_reward']:.2f}")
    
    print("3. Save model...")
    agent.save_model()
    
    print("4. Evaluate model...")
    avg_reward, avg_metrics = agent.evaluate(num_episodes=2)
    print(f"   Evaluation: Avg Reward = {avg_reward:.2f}")
    
    print("5. Load model...")
    agent.load_model()
    
    print("\nâœ“ Training workflow example completed!")

def main():
    """ä¸»å‡½æ•°"""
    print("STH (Spatio-Temporal Hybrid Attention Network) Model Examples")
    print("This demonstrates the usage of STH model for traffic signal control")
    
    try:
        # è¿è¡Œå„ç§ç¤ºä¾‹
        example_basic_usage()
        example_model_inference()
        example_custom_config()
        example_training_workflow()
        
        print("\n" + "=" * 60)
        print("ğŸ‰ All examples completed successfully!")
        print("=" * 60)
        
        print("\nNext steps:")
        print("1. Replace MockTrafficEnvironment with your actual traffic simulation environment")
        print("2. Adjust the configuration parameters based on your specific use case")
        print("3. Run the training with your environment")
        print("4. Monitor the training progress and adjust hyperparameters as needed")
        
    except Exception as e:
        print(f"\nâŒ Error occurred: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main() 